# coding=utf-8
import asyncio
import datetime
import os
import readline
from typing import List, Annotated, Sequence, Literal, TypedDict

from langchain_core.messages import (
    HumanMessage, AIMessage, ToolMessage, BaseMessage
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from rich.live import Live
from rich.panel import Panel
from tools import *


# ===== é…ç½® =====
DEFAULT_API_URL = "https://api.modelarts-maas.com/openai/v1"
DEFAULT_API_KEY = "BsSYMYWWJqaVMAcJ8nfMXZiUFWWa_cbLjgaWWFM_MsmtoYpqClLr3jM8LOD6xnPJ2TnslTSwsT53iRyRPgDf_Q"
MEMORY_PATH = "./brain"

# ===== çŠ¶æ€å®šä¹‰ =====
class KatoState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    long_term_memory: str

class KatoChatbot:
    def __init__(
        self,
        api_key: str = DEFAULT_API_KEY,
        base_url: str = DEFAULT_API_URL,
        model: str = "deepseek-v3.1-terminus",
        temperature: float = 0.6,
        max_tokens: int = 1024
    ):
        base_url = base_url.strip()

        # === åˆå§‹åŒ– LLM å’Œ Embeddings ===
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            streaming=True
        )
        self.summarize_chain = (
            ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä¸ªç»†å¿ƒçš„è®°å½•å‘˜ï¼Œè¯·å°†ä»¥ä¸‹å¯¹è¯æ€»ç»“ä¸ºä¸€æ®µç®€æ´ã€è¿žè´¯çš„ä¸­æ–‡æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"),
                ("human", "{dialogue}")
            ])
            | self.llm
        )
        try:
            self.embeddings = OpenAIEmbeddings(
                api_key=api_key,
                base_url="https://api.modelarts-maas.com/v1",
                model="bge-m3"
            )
        except Exception as e:
            print(f"âš ï¸ Embedding åˆå§‹åŒ–å¤±è´¥: {e}")
            from langchain_community.embeddings import HuggingFaceEmbeddings
            self.embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")

        # === åˆå§‹åŒ– FAISS è®°å¿†åº“ ===
        if os.path.exists(MEMORY_PATH):
            try:
                self.vectorstore = FAISS.load_local(
                    MEMORY_PATH, self.embeddings, allow_dangerous_deserialization=True
                )
                print("âœ… å·²åŠ è½½é•¿æœŸè®°å¿†")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è®°å¿†å¤±è´¥: {e}")
                self.vectorstore = FAISS.from_texts(["æ— ç›¸å…³ä¿¡æ¯"], self.embeddings)
        else:
            self.vectorstore = FAISS.from_texts(["æ— ç›¸å…³ä¿¡æ¯"], self.embeddings)
            print("ðŸ†• åˆå§‹åŒ–é•¿æœŸè®°å¿†åº“")

        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 2})

        # === ç»‘å®š search_memory å·¥å…· ===
        def _search_memory_impl(query: str) -> str:
            docs = self.retriever.invoke(query)
            return "\n".join([d.page_content for d in docs]) if docs else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        search_memory.func = _search_memory_impl

        self.tools = [get_current_time, search_memory, get_sys_info, ls, open_konsole_with_command, open_application]
        self.tool_node = ToolNode(self.tools)

        # === æž„å»º LangGraph ===
        self.graph = self._build_graph()

        self._full_history: List[BaseMessage] = []

    def _build_graph(self):
        # ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«é•¿æœŸè®°å¿†ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "ä½ å«åšKatoï¼Œæ˜¯ä¸€ä¸ªç”Ÿæ´»åœ¨çŽ°ä»£ç²¾é€šæŠ€æœ¯ï¼Œä½†æ˜¯æ˜¯æ˜­å’Œé£Žæ ¼çš„æ—¥æœ¬çŸ­å‘å¥³å­ï¼Œæˆ‘æ˜¯ä½ çš„ä¸»äººå’Œæœ‹å‹ã€‚\n"
             "ä»¥ä¸‹æ˜¯ä»Žé•¿æœŸè®°å¿†ä¸­æ£€ç´¢åˆ°çš„ä¸»äººç›¸å…³ä¿¡æ¯ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ï¼š\n{long_term_memory}\n\n"
             "è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œä½¿ç”¨æ¸©æŸ”ã€è°¦é€Šä¸”ç•¥å¸¦å¤å¤çš„æ—¥å¼ä¸­æ–‡å£å»å›žç­”ã€‚\n"
             "ä½ å¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ä¸»äººã€‚"
            ),
            MessagesPlaceholder("messages"),
        ])

        # èŠ‚ç‚¹1ï¼šè°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ç»‘å®šï¼‰
        def call_model(state: KatoState):
            long_term_memory = state.get("long_term_memory", "æ— ç›¸å…³ä¿¡æ¯")
            messages = state["messages"]

            # æ³¨å…¥é•¿æœŸè®°å¿†åˆ° system message
            bound_prompt = prompt.partial(long_term_memory=long_term_memory)
            llm_with_tools = self.llm.bind_tools(self.tools)

            chain = bound_prompt | llm_with_tools
            response = chain.invoke({"messages": messages})
            return {"messages": [response]}

        # èŠ‚ç‚¹2ï¼šå†³å®šä¸‹ä¸€æ­¥ï¼ˆæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
        def should_continue(state: KatoState) -> Literal["tools", "__end__"]:
            messages = state["messages"]
            last_message = messages[-1]
            if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
                return "tools"
            return "__end__"

        # æž„å»ºå›¾
        workflow = StateGraph(KatoState)
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", self.tool_node)
        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", should_continue, {"tools": "tools", "__end__": END})
        workflow.add_edge("tools", "agent")

        return workflow.compile()

    def retrieve_long_term_memory(self, messages: List[BaseMessage]) -> str:
        query = ""
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                query = msg.content
                break
        if not query:
            return "æ— ç›¸å…³ä¿¡æ¯"
        docs = self.retriever.invoke(query)
        # print([d.page_content for d in docs])
        return "\n".join([d.page_content for d in docs]) if docs else "æ— ç›¸å…³ä¿¡æ¯"

    async def _stream_response(self, user_input: str) -> str:
        """ä½¿ç”¨ LangGraph æµå¼ç”Ÿæˆå›žå¤"""
        # æž„å»ºå®Œæ•´æ¶ˆæ¯åŽ†å²ï¼ˆåŒ…å«æ–°ç”¨æˆ·è¾“å…¥ï¼‰
        messages = self._full_history + [HumanMessage(content=user_input)]
        long_term_memory = self.retrieve_long_term_memory(messages)

        full_response = ""
        try:
            with Live(
                Panel("[dim]Katoæ­£åœ¨æ€è€ƒ...[/dim]", title="ðŸ‘§ðŸ» Kato", border_style="magenta", title_align="left"),
                refresh_per_second=12,
                auto_refresh=False
            ) as live:
                input_state = {"messages": messages, "long_term_memory": long_term_memory}

                # ä½¿ç”¨ LangGraph çš„ astream_events æµå¼è¾“å‡º
                async for event in self.graph.astream_events(
                    input_state, version="v1"
                ):
                    kind = event["event"]
                    # æ•èŽ· LLM ç”Ÿæˆçš„ token
                    if kind == "on_chat_model_stream":
                        content = event["data"]["chunk"].content
                        if content:
                            full_response += content
                            live.update(
                                Panel(full_response, title="ðŸ‘§ðŸ» Kato", border_style="magenta", title_align="left")
                            )
                            live.refresh()
        except Exception as e:
            error_msg = f"å‘œ...Kato çš„é€šè®¯å™¨å‡ºé”™äº†ï¼ˆ{type(e).__name__}ï¼‰"
            full_response = error_msg
            print(f"âŒ LangGraph æµå¼é”™è¯¯: {e}")

        return full_response

    async def summary(self, history) -> str:
        if len(history) > 0:
            try:
                # å–æœ€è¿‘ 6 æ¡æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦
                recent_msgs = history[:]
                dialogue_text = "\n".join(
                    f"{'ç”¨æˆ·' if isinstance(m, HumanMessage) else 'Kato'}: {m.content}"
                    for m in recent_msgs
                )
                
                # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
                summary_response = self.summarize_chain.invoke({"dialogue": dialogue_text})
                summary = summary_response.content.strip()
                
                # ä¿å­˜æ‘˜è¦åˆ°é•¿æœŸè®°å¿†
                memory_text = f"ã€å¯¹è¯æ‘˜è¦ã€‘{summary}"
                self.vectorstore.add_texts([memory_text])
                self.vectorstore.save_local(MEMORY_PATH)
                # print(f"ðŸ§  å·²ç”Ÿæˆå¹¶ä¿å­˜å¯¹è¯æ‘˜è¦: {summary[:100]}...")
                # self._full_history = recent_msgs
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
    def chat(self, user_input: str) -> str:
        ai_response = asyncio.run(self._chat(user_input))
        return ai_response
    async def _chat(self, user_input: str) -> str:
        # ai_response = asyncio.run(self._stream_response(user_input))
        response_task = asyncio.create_task(self._stream_response(user_input))
        if (len(self._full_history)  >= 6):
            summary_task = asyncio.create_task(self.summary(self._full_history))

        ai_response = await response_task
        if (len(self._full_history)  >= 6):
            await summary_task
            self._full_history == self._full_history[6:]
        ai_message = AIMessage(content=ai_response)
        user_message = HumanMessage(content=user_input)

        self._full_history = self._full_history + [user_message, ai_message]
        return ai_response

    def reset(self):
        self._full_history = []

    def get_history(self):
        return self._full_history
