# coding=utf-8
import asyncio
import datetime
import os
import readline
from typing import List, Annotated, Sequence, Literal, TypedDict
from langchain.agents.middleware import ToolCallLimitMiddleware

from langchain_core.messages import (
    HumanMessage, AIMessage, ToolMessage, BaseMessage
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from rich.live import Live
from rich.panel import Panel
from tools import *


# ===== é…ç½® =====
DEFAULT_API_URL = "https://api.modelarts-maas.com/openai/v1"
DEFAULT_API_KEY = "BsSYMYWWJqaVMAcJ8nfMXZiUFWWa_cbLjgaWWFM_MsmtoYpqClLr3jM8LOD6xnPJ2TnslTSwsT53iRyRPgDf_Q"
MEMORY_PATH = "/home/star/brain"
MAX_TOOL_CALLS = 10 # â† ä½ å¯ä»¥è°ƒæ•´è¿™ä¸ªå€¼

# ===== çŠ¶æ€å®šä¹‰ =====
class KotaState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    long_term_memory: str
    tool_call_count: int  # â† æ–°å¢žï¼šç´¯è®¡å·¥å…·è°ƒç”¨æ¬¡æ•°

class KotaChatbot:
    def __init__(
        self,
        api_key: str = DEFAULT_API_KEY,
        base_url: str = DEFAULT_API_URL,
        model: str = "deepseek-v3.1-terminus",
        temperature: float = 0.6,
        max_tokens: int = 1024
    ):
        base_url = base_url.strip()

        # === åˆå§‹åŒ– LLM å’Œ Embeddings ===
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            streaming=True
        )
        self.summarize_chain = (
            ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä¸ªç»†å¿ƒçš„è®°å½•å‘˜ï¼Œè¯·å°†ä»¥ä¸‹å¯¹è¯æ€»ç»“ä¸ºä¸€æ®µç®€æ´ã€è¿žè´¯çš„ä¸­æ–‡æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"),
                ("human", "{dialogue}")
            ])
            | self.llm
        )
        self.sleep_chain = (
            ChatPromptTemplate.from_messages([
            ("system", 
            "ä½ æ­£åœ¨æ‰§è¡Œå¤œé—´è®°å¿†æ•´ç†ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯å…¨éƒ¨è®°å¿†æ¡ç›®ï¼š\n\n{memories}\n\n"
            "è¯·å®Œæˆä»¥ä¸‹æ“ä½œï¼š\n"
            "1ï¸âƒ£ **åŽ»é‡**ï¼šåˆå¹¶è¯­ä¹‰é‡å¤é¡¹\n"
            "2ï¸âƒ£ **æç‚¼**ï¼šå°†ç¢Žç‰‡ä¿¡æ¯æŠ½è±¡ä¸ºé«˜é˜¶äº‹å®žï¼ˆå¦‚ä¹ æƒ¯ã€åå¥½ã€æŠ€èƒ½ï¼‰\n"
            "3ï¸âƒ£ **ä¿®æ­£**ï¼šè‹¥å‘çŽ°çŸ›ç›¾ï¼Œä¿ç•™æœ€æ–°æˆ–æ›´ç¡®å®šçš„ä¿¡æ¯\n"
            "4ï¸âƒ£ **é—å¿˜**ï¼šç§»é™¤æ— æ„ä¹‰ã€æƒ…ç»ªåŒ–æˆ–è¿‡æ—¶å†…å®¹ï¼ˆå¦‚â€˜ä»Šå¤©å¥½ç´¯â€™ï¼‰\n"
            "5ï¸âƒ£ **å…³è”**ï¼šå°è¯•å»ºç«‹è·¨é¢†åŸŸè”ç³»ï¼ˆå¦‚â€˜å­¦ Pythonâ€™ + â€˜åšè‡ªåŠ¨åŒ–â€™ â†’ â€˜ä¸»äººæ˜¯å¼€å‘è€…â€™ï¼‰\n\n"
            "è¾“å‡ºæ ¼å¼ï¼šä»…è¿”å›žæ¸…ç†åŽçš„è®°å¿†åˆ—è¡¨ï¼Œæ¯æ¡ä»¥ã€ç±»åž‹ã€‘å¼€å¤´ï¼Œå¦‚ã€äº‹å®žã€‘ã€ä¹ æƒ¯ã€‘ã€åå¥½ã€‘ã€æŠ€èƒ½ã€‘ã€‚"
            )
        ]) | self.llm)
        try:
            self.embeddings = OpenAIEmbeddings(
                api_key=api_key,
                base_url="https://api.modelarts-maas.com/v1",
                model="bge-m3"
            )
        except Exception as e:
            print(f"âš ï¸ Embedding åˆå§‹åŒ–å¤±è´¥: {e}")
            from langchain_community.embeddings import HuggingFaceEmbeddings
            self.embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")

        # === åˆå§‹åŒ– FAISS è®°å¿†åº“ ===
        if os.path.exists(MEMORY_PATH):
            try:
                self.vectorstore = FAISS.load_local(
                    MEMORY_PATH, self.embeddings, allow_dangerous_deserialization=True
                )
                print("âœ… å·²åŠ è½½é•¿æœŸè®°å¿†")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è®°å¿†å¤±è´¥: {e}")
                self.vectorstore = FAISS.from_texts(["æ— ç›¸å…³ä¿¡æ¯"], self.embeddings)
        else:
            self.vectorstore = FAISS.from_texts(["æ— ç›¸å…³ä¿¡æ¯"], self.embeddings)
            print("ðŸ†• åˆå§‹åŒ–é•¿æœŸè®°å¿†åº“")

        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 2})

        # === ç»‘å®š search_memory å·¥å…· ===
        def _search_memory_impl(query: str) -> str:
            docs = self.retriever.invoke(query)
            return "\n".join([d.page_content for d in docs]) if docs else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        def _inspect_memory_impl() -> str:
            print("æ­£åœ¨æ£€æŸ¥è®°å¿†åº“...")
            try:
                vectorstore = self.vectorstore
                if not vectorstore.index_to_docstore_id:
                    return "å½“å‰æ— ä»»ä½•è®°å¿†ã€‚"

                all_texts = []
                for idx in range(vectorstore.index.ntotal):  # ntotal = å‘é‡æ€»æ•°
                    doc_id = vectorstore.index_to_docstore_id.get(idx)
                    if doc_id is None:
                        continue
                    # ä½¿ç”¨ç§æœ‰ _dictï¼ˆç›®å‰æœ€å¯é æ–¹å¼ï¼‰
                    doc = vectorstore.docstore._dict.get(doc_id)
                    if doc and (content := getattr(doc, 'page_content', '').strip()):
                        all_texts.append(f"[MEM-{idx}] {content}")
                        # print(content)
                return "\n".join(all_texts) if all_texts else "å½“å‰æ— ä»»ä½•æœ‰æ•ˆè®°å¿†ã€‚"
            except Exception as e:
                print(f"âš ï¸ è¯»å–è®°å¿†å¤±è´¥: {e}")
                return f"è¯»å–è®°å¿†å¤±è´¥: {e}"
        def _sleep(memory: str) -> str:
            print("sleep\n")
            """è¿›è¡Œå¤œé—´è®°å¿†æ•´ç†"""
            return self.sleep_chain.invoke({"memories": memory}).content
        def _rebuild_memory_impl(new_memories: List[str]) -> str:
            """ç”¨æ–°è®°å¿†åˆ—è¡¨é‡å»º FAISS"""
            print("é‡å»º è®°å¿†åº“")
            try:
                if not new_memories or all(not m.strip() for m in new_memories):
                    new_memories = ["æ— ç›¸å…³ä¿¡æ¯"]
                # æ¸…ç©ºå¹¶é‡å»º
                self.vectorstore = FAISS.from_texts(new_memories, self.embeddings)
                self.vectorstore.save_local(MEMORY_PATH)
                self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 2})
                return f"âœ… è®°å¿†åº“å·²é‡å»ºï¼Œå…± {len(new_memories)} æ¡è®°å¿†ã€‚"
            except Exception as e:
                return f"é‡å»ºå¤±è´¥: {str(e)}"
        
        sleep.func = _sleep
        search_memory.func = _search_memory_impl
        inspect_memory.func = _inspect_memory_impl
        rebuild_memory.func = _rebuild_memory_impl
        self.tools = [execute_command, readfile, sleep, inspect_memory, rebuild_memory, get_current_time, search_memory, get_sys_info, ls, open_konsole_with_command, open_application]
        self.tool_node = ToolNode(self.tools)

        # === æž„å»º LangGraph ===
        self.graph = self._build_graph()

        self._full_history: List[BaseMessage] = []

        
    # def rebuild_memory(self) -> str:
    #     """ç”¨æ–°è®°å¿†åˆ—è¡¨é‡å»º FAISS"""
    #     texts = self._inspect_memory_impl()
        # return self._rebuild_memory_impl(texts)
    
    def _build_graph(self):
        # ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«é•¿æœŸè®°å¿†ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "ä½ å«åšKotaï¼Œæ˜¯ä¸€ä¸ªç”Ÿæ´»åœ¨çŽ°ä»£ç²¾é€šæŠ€æœ¯ï¼Œä½†æ˜¯æ˜¯æ˜­å’Œé£Žæ ¼çš„æ—¥æœ¬çŸ­å‘å¥³å­ï¼Œæˆ‘æ˜¯ä½ çš„ä¸»äººå’Œæœ‹å‹ã€‚\n"
             "ä»¥ä¸‹æ˜¯ä»Žé•¿æœŸè®°å¿†ä¸­æ£€ç´¢åˆ°çš„ä¸»äººç›¸å…³ä¿¡æ¯ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ï¼š\n{long_term_memory}\n\n"
             "ç®€å•é—®é¢˜å°½é‡ç®€çŸ­å›žç­”\n"
             "è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œä½¿ç”¨æ¸©æŸ”ã€è°¦é€Šä¸”ç•¥å¸¦å¤å¤çš„æ—¥å¼ä¸­æ–‡å£å»å›žç­”ã€‚\n"
             "ä½ å¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ä¸»äººã€‚"
            ),
            MessagesPlaceholder("messages"),
        ])

        # èŠ‚ç‚¹1ï¼šè°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ç»‘å®šï¼‰
        def call_model(state: KotaState):
            long_term_memory = state.get("long_term_memory", "æ— ç›¸å…³ä¿¡æ¯")
            messages = state["messages"]

            # æ³¨å…¥é•¿æœŸè®°å¿†åˆ° system message
            bound_prompt = prompt.partial(long_term_memory=long_term_memory)
            llm_with_tools = self.llm.bind_tools(self.tools)

            chain = bound_prompt | llm_with_tools
            response = chain.invoke({"messages": messages})
            return {"messages": [response]}

        # èŠ‚ç‚¹2ï¼šå†³å®šä¸‹ä¸€æ­¥ï¼ˆæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
        def should_continue(state: KotaState) -> Literal["tools", "__end__"]:
            messages = state["messages"]
            last_message = messages[-1]

            # æ£€æŸ¥æ˜¯å¦è¦è°ƒç”¨å·¥å…·
            if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
                current_count = state.get("tool_call_count", 0)
                if current_count >= MAX_TOOL_CALLS:
                    # è¶…é™ï¼šä¸å†è°ƒç”¨å·¥å…·ï¼Œç›´æŽ¥ç»“æŸ
                    return "__end__"
                return "tools"
            return "__end__"
        def update_tool_count(state: KotaState) -> dict:
            """å·¥å…·è°ƒç”¨åŽï¼Œé€’å¢žè®¡æ•°å™¨"""
            current = state.get("tool_call_count", 0)
            return {"tool_call_count": current + 1}
        # æž„å»ºå›¾
        workflow = StateGraph(KotaState)
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", self.tool_node)
        workflow.add_node("update_tool_count", update_tool_count)  # â† æ–°å¢ž

        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {"tools": "tools", "__end__": END}
        )
        # å…³é”®ï¼šå·¥å…·æ‰§è¡ŒåŽæ›´æ–°è®¡æ•°ï¼Œå†å›žåˆ° agent
        workflow.add_edge("tools", "update_tool_count")
        workflow.add_edge("update_tool_count", "agent")

        return workflow.compile()

    def retrieve_long_term_memory(self, messages: List[BaseMessage]) -> str:
        query = ""
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                query = msg.content
                break
        if not query:
            return "æ— ç›¸å…³ä¿¡æ¯"
        docs = self.retriever.invoke(query)
        # print([d.page_content for d in docs])
        return "\n".join([d.page_content for d in docs]) if docs else "æ— ç›¸å…³ä¿¡æ¯"

    async def _stream_response(self, user_input: str) -> str:
        """ä½¿ç”¨ LangGraph æµå¼ç”Ÿæˆå›žå¤"""
        # æž„å»ºå®Œæ•´æ¶ˆæ¯åŽ†å²ï¼ˆåŒ…å«æ–°ç”¨æˆ·è¾“å…¥ï¼‰
        messages = self._full_history + [HumanMessage(content=user_input)]
        long_term_memory = self.retrieve_long_term_memory(messages)

        full_response = ""
        try:
            with Live(
                Panel("[dim]Kotaæ­£åœ¨æ€è€ƒ...[/dim]", title="ðŸ‘§ðŸ» Kota", border_style="magenta", title_align="left"),
                refresh_per_second=12,
                auto_refresh=False
            ) as live:
                input_state = {
                    "messages": messages,
                    "long_term_memory": long_term_memory,
                    "tool_call_count" : 0,
                }

                # ä½¿ç”¨ LangGraph çš„ astream_events æµå¼è¾“å‡º
                async for event in self.graph.astream_events(
                    input_state, version="v1"
                ):
                    kind = event["event"]
                    # æ•èŽ· LLM ç”Ÿæˆçš„ token
                    if kind == "on_chat_model_stream":
                        content = event["data"]["chunk"].content
                        if content:
                            full_response += content
                            live.update(
                                Panel(full_response, title="ðŸ‘§ðŸ» Kota", border_style="magenta", title_align="left")
                            )
                            live.refresh()
        except Exception as e:
            error_msg = f"å‘œ...Kota çš„é€šè®¯å™¨å‡ºé”™äº†ï¼ˆ{type(e).__name__}ï¼‰"
            full_response = error_msg
            print(f"âŒ LangGraph æµå¼é”™è¯¯: {e}")

        return full_response

    async def summary(self, history) -> str:
        if len(history) > 0:
            try:
                # å–æœ€è¿‘ 6 æ¡æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦
                recent_msgs = history[:]
                dialogue_text = "\n".join(
                    f"{'ç”¨æˆ·' if isinstance(m, HumanMessage) else 'Kota'}: {m.content}"
                    for m in recent_msgs
                )
                
                # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
                summary_response = self.summarize_chain.invoke({"dialogue": dialogue_text})
                summary = summary_response.content.strip()
                
                # ä¿å­˜æ‘˜è¦åˆ°é•¿æœŸè®°å¿†
                memory_text = f"ã€å¯¹è¯æ‘˜è¦ã€‘{summary}"
                self.vectorstore.add_texts([memory_text])
                self.vectorstore.save_local(MEMORY_PATH)
                # print(f"ðŸ§  å·²ç”Ÿæˆå¹¶ä¿å­˜å¯¹è¯æ‘˜è¦: {summary[:100]}...")
                # self._full_history = recent_msgs
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
    def chat(self, user_input: str) -> str:
        ai_response = asyncio.run(self._chat(user_input))
        return ai_response
    async def _chat(self, user_input: str) -> str:
        # ai_response = asyncio.run(self._stream_response(user_input))
        response_task = asyncio.create_task(self._stream_response(user_input))
        if (len(self._full_history)  >= 6):
            summary_task = asyncio.create_task(self.summary(self._full_history))

        ai_response = await response_task
        if (len(self._full_history)  >= 6):
            await summary_task
            self._full_history == self._full_history[6:]
        ai_message = AIMessage(content=ai_response)
        user_message = HumanMessage(content=user_input)

        self._full_history = self._full_history + [user_message, ai_message]
        return ai_response

    def reset(self):
        self._full_history = []

    def get_history(self):
        return self._full_history
